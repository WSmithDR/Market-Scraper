## üß≠ Gu√≠a de Desarrollo del Proyecto "Market Scraper"

Este proyecto se puede dividir en 5 fases principales, siguiendo una metodolog√≠a √°gil/iterativa, lo que permite la entrega continua de valor. 
---

### 1. ‚öôÔ∏è Fase de Configuraci√≥n y Planificaci√≥n (Sprints 1-2)

#### 1.1. Gesti√≥n del Proyecto y Colaboraci√≥n
* **Metodolog√≠a:** **Scrum** o **Kanban** simple.
* **Herramientas:** **Trello** o **Jira** (para tareas y *backlog*), **Git/GitHub/GitLab** (para control de versiones).
* **Entregable Clave:** Repositorio inicial con estructura de proyecto (carpetas `src/`, `data/`, `notebooks/`, `docs/`).

#### 1.2. Pila Tecnol√≥gica Central
| Componente | Tecnolog√≠a Sugerida | Raz√≥n |
| :--- | :--- | :--- |
| **Lenguaje** | **Python** | Ideal para Data Science, *Scraping* (Scrapy), y *Machine Learning*. |
| **Scraping** | **Scrapy** y/o **Selenium** | Scrapy para sitios est√°ticos/API. Selenium para sitios din√°micos (e.g., LinkedIn). |
| **Bases de Datos** | **MongoDB** (Principal) y **Supabase** (Reporte/API) | MongoDB es flexible para datos semi-estructurados de *scraping*. Supabase (PostgreSQL) es ideal para el almacenamiento estructurado final y la API. |
| **Orquestaci√≥n** | **Docker** y **Docker Compose** | Empaquetar el *scraper* y la base de datos para garantizar la portabilidad y la f√°cil programaci√≥n. |

---

### 2. üé£ Fase de Extracci√≥n de Datos (*Scraping*) (Sprints 2-3)

Esta fase es el n√∫cleo del **Market Scraper**.

#### 2.1. Desarrollo del Scraper
* **Manejo de Sitios:**
    * **Sitios Est√°ticos (Bolsas de Empleo):** Utilizar **Scrapy** o **BeautifulSoup** con el m√≥dulo `requests`.
    * **Sitios Din√°micos (LinkedIn, Sitios de Empresas):** Utilizar **Selenium** o **Playwright** para interactuar con JavaScript y simular la navegaci√≥n del usuario.
* **Mecanismos de Control (Anti-Bloqueo):**
    * Implementar *Proxies Rotativos* o usar servicios *Scraping as a Service* (e.g., ScraperAPI) si los bloqueos son frecuentes.
    * Configurar *delays* (**Scrapy AUTOTHROTTLE**) para simular un comportamiento humano y cumplir con la pol√≠tica de uso del sitio.
    * Implementar gesti√≥n de *cookies* y *headers* de sesi√≥n.
* **Extracci√≥n de Informaci√≥n de la Empresa (Enriquecimiento):**
    * Despu√©s de obtener el nombre de la empresa de una vacante, el *scraper* debe hacer una consulta secundaria (ej. una API como Clearbit o simplemente Google/LinkedIn) para obtener el **tama√±o, la industria, y el contacto general**.

#### 2.2. Programaci√≥n y Orquestaci√≥n
* El Full Stack Developer deber√≠a usar **Docker** para crear una imagen del *scraper* con todas las dependencias.
* **Programaci√≥n (Scheduling):** Usar **Docker Compose** o una herramienta de orquestaci√≥n (como **Apache Airflow** o un simple **Cron Job** dentro de un contenedor) para ejecutar el *scraper* a intervalos definidos.

---

### 3. ‚ú® Fase ETL (*Extraction, Transformation, Loading*) (Sprints 3-4)

Los Cient√≠ficos de Datos liderar√°n esta fase, esencial para la calidad del an√°lisis.

#### 3.1. Pipeline de Limpieza (T de Transformaci√≥n)
* **Normalizaci√≥n de Texto:**
    * **Roles:** Normalizar nombres de roles (ej. "Data Sci", "Data Scientist", "Cient√≠fico de Datos" $\rightarrow$ **Cient√≠fico de Datos**).
    * **Tecnolog√≠as:** Extracci√≥n y normalizaci√≥n de *skills* (ej. "Pyth", "Pithon" $\rightarrow$ **Python**). Usar **expresiones regulares (regex)** y **NLP b√°sico**.
* **Limpieza de Datos:** Eliminar duplicados bas√°ndose en una combinaci√≥n de t√≠tulo, empresa y ubicaci√≥n.
* **Estandarizaci√≥n:** Convertir los niveles de *seniority* (Junior, Mid, Senior) a un formato consistente.
* **Filtrado:** Aplicar los filtros requeridos (pa√≠s, industria, etc.) en esta etapa.

#### 3.2. Almacenamiento Estructurado (L de Loading)
* Los datos limpios deben migrarse de la base de datos inicial (MongoDB) a la base de datos final **Supabase (PostgreSQL)** en un esquema tabular bien definido.
* **Esquema de Tabla Sugerido:**

| Campo | Tipo de Dato | Prop√≥sito |
| :--- | :--- | :--- |
| `id_vacante` | UUID | Identificador √önico |
| `titulo_normalizado` | String | T√≠tulo del rol estandarizado |
| `empresa_nombre` | String | Nombre de la empresa |
| `empresa_tamano` | String | (Peque√±a, Mediana, Grande) |
| `pais` | String | Ubicaci√≥n (normalizado) |
| `seniority` | String | Nivel de experiencia (J, M, S) |
| `fecha_extraccion` | Timestamp | Cu√°ndo se extrajo el dato |
| `skills_list` | Array de Strings | Lista de tecnolog√≠as clave extra√≠das |

---

### 4. üìä Fase de An√°lisis y Reporte (Sprints 4-5)

Esta fase es totalmente de dominio de los Cient√≠ficos de Datos.

#### 4.1. An√°lisis de Tendencias
* **Herramientas:** **Jupyter Notebooks** (con `pandas`, `numpy`), y **Plotly** o **Matplotlib** para visualizaci√≥n.
* **M√©tricas Clave:**
    * **Crecimiento de Demanda:** Calcular la variaci√≥n porcentual de vacantes por rol/skill en el tiempo.
    * ***Skills* Emergentes:** Identificar las *skills* con el mayor aumento de menciones en el √∫ltimo periodo.
    * **Distribuci√≥n Geogr√°fica/Sectorial:** Mapa de calor de la demanda de roles espec√≠ficos.
* **Integraci√≥n:** Usar la API de Supabase para consultar directamente los datos limpios.

#### 4.2. Entregable de Reporte
* **Opciones de Dashboard/Notebook:**
    * **Jupyter Notebook:** Para un reporte t√©cnico.
    * **Streamlit/Dash:** El Full Stack Developer puede construir un *dashboard* simple y r√°pido con estas herramientas, consumiendo la **API de Supabase**. Esto permite un reporte autom√°tico y din√°mico.

---

### 5. üìö Fase de Documentaci√≥n y Entrega (Sprint 5)

* **Documentaci√≥n del C√≥digo:**
    * Documentar el *scraper* (`README.md`) con instrucciones claras sobre c√≥mo instalar, configurar variables de entorno y ejecutarlo con Docker.
    * Utilizar *docstrings* en el c√≥digo Python.
* **Documentaci√≥n del An√°lisis:** Asegurar que el *notebook* de *insights* contenga las conclusiones y la interpretaci√≥n de las tendencias.
* **Entrega Final:** Los entregables solicitados son:
    * **Dataset de 500+ registros procesados:** Entregado en la base de datos Supabase y exportable (CSV/JSON).
    * **Script documentado y automatizable:** Repositorio en GitHub con el c√≥digo Python y el `Dockerfile`.
    * **Reporte de insights:** Notebook o dashboard en Streamlit/Dash.

---

## üõ†Ô∏è Conclusi√≥n de la Pila Tecnol√≥gica

| Rol | Foco Principal | Tecnolog√≠as Clave |
| :--- | :--- | :--- |
| **Cient√≠ficos de Datos** | Desarrollo del Scraper, ETL, An√°lisis de Tendencias | **Python**, **Scrapy**, **Pandas**, **Regex**, **MongoDB** (Input), **Supabase** (Output) |
| **Full Stack Developer** | Infraestructura, Orquestaci√≥n, Dashboard de Reporte | **Docker**, **Airflow/Cron**, **Supabase API**, **Streamlit/Dash** |

La combinaci√≥n de **Python/Scrapy** para la recolecci√≥n, **MongoDB** para la ingesta flexible, **Supabase/PostgreSQL** para el almacenamiento estructurado final, y **Docker** para la automatizaci√≥n, proporciona una soluci√≥n robusta y escalable que se alinea perfectamente con las habilidades de su equipo.