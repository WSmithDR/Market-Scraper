{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b23114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-jobspy in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (1.1.82)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2.3.3)\n",
      "Requirement already satisfied: markdownify<0.14.0,>=0.13.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (0.13.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2.32.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.3.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2.12.4)\n",
      "Requirement already satisfied: NUMPY==1.26.3 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (1.26.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (4.14.2)\n",
      "Requirement already satisfied: tls-client<2.0.0,>=1.0.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (1.0.1)\n",
      "Requirement already satisfied: regex<2025.0.0,>=2024.4.28 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2024.11.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->python-jobspy) (4.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->python-jobspy) (2.8)\n",
      "Requirement already satisfied: six<2,>=1.15 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from markdownify<0.14.0,>=0.13.1->python-jobspy) (1.17.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2025.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (2.41.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2025.11.12)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (4.67.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xlsxwriter in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (3.2.9)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install needed libraries\n",
    "%pip install -U python-jobspy\n",
    "%pip install tqdm\n",
    "%pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14676f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from jobspy import scrape_jobs\n",
    "from pathlib import Path \n",
    "from datetime import datetime\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901c87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_directory(base_dir=\"../data/raw\"):\n",
    "    \"\"\"\n",
    "    Crea un directorio de salida con timestamp (usando pathlib) \n",
    "    y devuelve un objeto Path.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 2. Usar pathlib para crear la ruta\n",
    "    output_dir = Path(base_dir) / f\"jobs_{timestamp}\"\n",
    "    \n",
    "    # 3. Usar .mkdir() del objeto Path\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 4. Devolver el objeto Path (no un string)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fbf1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de salida: ../data/raw/jobs_20251115_171800\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Definir Directorio de Salida ---\n",
    "output_dir = setup_output_directory()\n",
    "print(f\"Directorio de salida: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415b9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Definir Par√°metros de B√∫squeda Base ---\n",
    "roles_buscados = [\"Data Scientist\", \"Machine Learning Engineer\", \"Data Analyst\"]\n",
    "sectores_clave = [\"Fintech\", \"EdTech\", \"Future of Work\"]\n",
    "search_terms = list(product(roles_buscados, sectores_clave))\n",
    "location = \"Remote\"\n",
    "country_code = \"USA\" # C√≥digo de pa√≠s para Indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c27f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par√°metros listos. Iniciaremos scrapers secuenciales y especializados.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Lista para guardar resultados ---\n",
    "# Guardaremos los DataFrames de cada sitio aqu√≠\n",
    "all_jobs_dfs = []\n",
    "\n",
    "print(\"Par√°metros listos. Iniciaremos scrapers secuenciales y especializados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef391493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: Indeed ---\n",
      "Buscando: \"Data Scientist\" AND \"Fintech\"\n",
      "‚úÖ Indeed encontr√≥ 12 trabajos.\n",
      "Buscando: \"Data Scientist\" AND \"EdTech\"\n",
      "Buscando: \"Data Scientist\" AND \"Future of Work\"\n",
      "‚úÖ Indeed encontr√≥ 3 trabajos.\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Fintech\"\n",
      "‚úÖ Indeed encontr√≥ 1 trabajos.\n",
      "Buscando: \"Machine Learning Engineer\" AND \"EdTech\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Future of Work\"\n",
      "‚úÖ Indeed encontr√≥ 14 trabajos.\n",
      "Buscando: \"Data Analyst\" AND \"Fintech\"\n",
      "‚úÖ Indeed encontr√≥ 5 trabajos.\n",
      "Buscando: \"Data Analyst\" AND \"EdTech\"\n",
      "Buscando: \"Data Analyst\" AND \"Future of Work\"\n",
      "‚úÖ Indeed encontr√≥ 1 trabajos.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Scraper: Indeed (El \"Caballo de batalla\") ---\n",
    "# Es el m√°s estable y sin l√≠mites de solicitudes\n",
    "print(\"\\n--- Iniciando Scraper: Indeed ---\")\n",
    "for item in search_terms:\n",
    "    search_term = f'\"{item[0]}\" AND \"{item[1]}\"'\n",
    "    print(f\"Buscando: {search_term}\")\n",
    "    try:\n",
    "        indeed_jobs = scrape_jobs(\n",
    "            site_name=[\"indeed\"],\n",
    "            search_term=search_term, # Podemos afinar el t√©rmino\n",
    "            location=location,\n",
    "            country_indeed=country_code,\n",
    "            results_wanted=200, # Le pedimos m√°s porque es estable\n",
    "            hours_old=720\n",
    "        )\n",
    "        if indeed_jobs is not None and not indeed_jobs.empty:\n",
    "            print(f\"‚úÖ Indeed encontr√≥ {len(indeed_jobs)} trabajos.\")\n",
    "            all_jobs_dfs.append(indeed_jobs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Indeed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c5b5638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: ZipRecruiter ---\n",
      "Buscando: \"Data Scientist\" AND \"Fintech\"\n",
      "Buscando: \"Data Scientist\" AND \"EdTech\"\n",
      "Buscando: \"Data Scientist\" AND \"Future of Work\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Fintech\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"EdTech\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Future of Work\"\n",
      "Buscando: \"Data Analyst\" AND \"Fintech\"\n",
      "Buscando: \"Data Analyst\" AND \"EdTech\"\n",
      "Buscando: \"Data Analyst\" AND \"Future of Work\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Scraper: ZipRecruiter (El \"Est√°ndar\") ---\n",
    "print(\"\\n--- Iniciando Scraper: ZipRecruiter ---\")\n",
    "for item in search_terms:\n",
    "    search_term = f'\"{item[0]}\" AND \"{item[1]}\"'\n",
    "    print(f\"Buscando: {search_term}\")\n",
    "    try:\n",
    "        zip_jobs = scrape_jobs(\n",
    "            site_name=[\"zip_recruiter\"],\n",
    "            search_term=search_term,\n",
    "            location=location,\n",
    "            results_wanted=100, \n",
    "            hours_old=720\n",
    "        )\n",
    "        if zip_jobs is not None and not zip_jobs.empty:\n",
    "            print(f\"‚úÖ ZipRecruiter encontr√≥ {len(zip_jobs)} trabajos.\")\n",
    "            all_jobs_dfs.append(zip_jobs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en ZipRecruiter: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc714ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: Glassdoor ---\n",
      "Buscando: \"Data Scientist\" AND \"Fintech\"\n",
      "‚úÖ GlassDoor encontr√≥ 21 trabajos.\n",
      "Buscando: \"Data Scientist\" AND \"EdTech\"\n",
      "Buscando: \"Data Scientist\" AND \"Future of Work\"\n",
      "‚úÖ GlassDoor encontr√≥ 4 trabajos.\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Fintech\"\n",
      "‚úÖ GlassDoor encontr√≥ 1 trabajos.\n",
      "Buscando: \"Machine Learning Engineer\" AND \"EdTech\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Future of Work\"\n",
      "‚úÖ GlassDoor encontr√≥ 10 trabajos.\n",
      "Buscando: \"Data Analyst\" AND \"Fintech\"\n",
      "‚úÖ GlassDoor encontr√≥ 8 trabajos.\n",
      "Buscando: \"Data Analyst\" AND \"EdTech\"\n",
      "Buscando: \"Data Analyst\" AND \"Future of Work\"\n",
      "‚úÖ GlassDoor encontr√≥ 1 trabajos.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Iniciando Scraper: Glassdoor ---\")\n",
    "for item in search_terms:\n",
    "    search_term = f'\"{item[0]}\" AND \"{item[1]}\"'\n",
    "    print(f\"Buscando: {search_term}\")\n",
    "    try:\n",
    "        zip_jobs = scrape_jobs(\n",
    "            site_name=[\"glassdoor\"],\n",
    "            search_term=search_term,\n",
    "            location=location,\n",
    "            results_wanted=100, \n",
    "            hours_old=720\n",
    "        )\n",
    "        if zip_jobs is not None and not zip_jobs.empty:\n",
    "            print(f\"‚úÖ GlassDoor encontr√≥ {len(zip_jobs)} trabajos.\")\n",
    "            all_jobs_dfs.append(zip_jobs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Glassdoor: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b710496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: Google ---\n",
      "Buscando: \"Data Scientist\" AND \"Fintech\"\n",
      "Buscando: \"Data Scientist\" AND \"EdTech\"\n",
      "Buscando: \"Data Scientist\" AND \"Future of Work\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Fintech\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"EdTech\"\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Future of Work\"\n",
      "Buscando: \"Data Analyst\" AND \"Fintech\"\n",
      "Buscando: \"Data Analyst\" AND \"EdTech\"\n",
      "Buscando: \"Data Analyst\" AND \"Future of Work\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Iniciando Scraper: Google ---\")\n",
    "for item in search_terms:\n",
    "    search_term = f'\"{item[0]}\" AND \"{item[1]}\"'\n",
    "    print(f\"Buscando: {search_term}\")\n",
    "    try:\n",
    "        zip_jobs = scrape_jobs(\n",
    "            site_name=[\"google\"],\n",
    "            search_term=search_term,\n",
    "            location=location,\n",
    "            results_wanted=100, \n",
    "            hours_old=720\n",
    "        )\n",
    "        if zip_jobs is not None and not zip_jobs.empty:\n",
    "            print(f\"‚úÖ Google encontr√≥ {len(zip_jobs)} trabajos.\")\n",
    "            all_jobs_dfs.append(zip_jobs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Google: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae56730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: Bayt ---\n",
      "Buscando: \"Data Scientist\" AND \"Fintech\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:37,189 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Data%20Scientist%22%20AND%20%22Fintech%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Data Scientist\" AND \"EdTech\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:37,531 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Data%20Scientist%22%20AND%20%22EdTech%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Data Scientist\" AND \"Future of Work\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:37,874 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Data%20Scientist%22%20AND%20%22Future%20of%20Work%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Machine Learning Engineer\" AND \"Fintech\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:38,198 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Machine%20Learning%20Engineer%22%20AND%20%22Fintech%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Machine Learning Engineer\" AND \"EdTech\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:38,532 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Machine%20Learning%20Engineer%22%20AND%20%22EdTech%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Machine Learning Engineer\" AND \"Future of Work\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:38,850 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Machine%20Learning%20Engineer%22%20AND%20%22Future%20of%20Work%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Data Analyst\" AND \"Fintech\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:39,208 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Data%20Analyst%22%20AND%20%22Fintech%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Data Analyst\" AND \"EdTech\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:39,592 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Data%20Analyst%22%20AND%20%22EdTech%22-jobs/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: \"Data Analyst\" AND \"Future of Work\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:18:39,959 - ERROR - JobSpy:Bayt - Bayt: Error fetching jobs - 403 Client Error: Forbidden for url: https://www.bayt.com/en/international/jobs/%22Data%20Analyst%22%20AND%20%22Future%20of%20Work%22-jobs/?page=1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Iniciando Scraper: Bayt ---\")\n",
    "for item in search_terms:\n",
    "    search_term = f'\"{item[0]}\" AND \"{item[1]}\"'\n",
    "    print(f\"Buscando: {search_term}\")\n",
    "    try:\n",
    "        zip_jobs = scrape_jobs(\n",
    "            site_name=[\"bayt\"],\n",
    "            search_term=search_term,\n",
    "            location=location,\n",
    "            results_wanted=100, \n",
    "            hours_old=720\n",
    "        )\n",
    "        if zip_jobs is not None and not zip_jobs.empty:\n",
    "            print(f\"‚úÖ Bayt encontr√≥ {len(zip_jobs)} trabajos.\")\n",
    "            all_jobs_dfs.append(zip_jobs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Bayt: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "406f5372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: bdjobs ---\n",
      "Buscando: \"Data Scientist\" AND \"Fintech\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Data Scientist\" AND \"EdTech\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Data Scientist\" AND \"Future of Work\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Fintech\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Machine Learning Engineer\" AND \"EdTech\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Future of Work\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Data Analyst\" AND \"Fintech\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Data Analyst\" AND \"EdTech\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n",
      "Buscando: \"Data Analyst\" AND \"Future of Work\"\n",
      "‚ùå Error en bdjobs: BDJobs.__init__() got an unexpected keyword argument 'user_agent'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Iniciando Scraper: bdjobs ---\")\n",
    "for item in search_terms:\n",
    "    search_term = f'\"{item[0]}\" AND \"{item[1]}\"'\n",
    "    print(f\"Buscando: {search_term}\")\n",
    "    try:\n",
    "        zip_jobs = scrape_jobs(\n",
    "            site_name=[\"bdjobs\"],\n",
    "            search_term=search_term,\n",
    "            location=location,\n",
    "            results_wanted=100, \n",
    "            hours_old=720\n",
    "        )\n",
    "        if zip_jobs is not None and not zip_jobs.empty:\n",
    "            print(f\"‚úÖ bdjobs encontr√≥ {len(zip_jobs)} trabajos.\")\n",
    "            all_jobs_dfs.append(zip_jobs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en bdjobs: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4cdedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: LinkedIn ---\n",
      "Buscando: \"Data Scientist\" AND \"Fintech\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:19:39,865 - INFO - JobSpy:Linkedin - finished scraping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LinkedIn encontr√≥ 50 trabajos.\n",
      "Buscando: \"Data Scientist\" AND \"EdTech\"\n",
      "‚úÖ LinkedIn encontr√≥ 9 trabajos.\n",
      "Buscando: \"Data Scientist\" AND \"Future of Work\"\n",
      "‚úÖ LinkedIn encontr√≥ 30 trabajos.\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Fintech\"\n",
      "‚úÖ LinkedIn encontr√≥ 40 trabajos.\n",
      "Buscando: \"Machine Learning Engineer\" AND \"EdTech\"\n",
      "‚ùå Error en LinkedIn: Invalid country string: 'sri lanka'. Valid countries are: argentina, australia, austria, bahrain, bangladesh, belgium, bulgaria, brazil, canada, chile, china, colombia, costa rica, croatia, cyprus, czech republic,czechia, denmark, ecuador, egypt, estonia, finland, france, germany, greece, hong kong, hungary, india, indonesia, ireland, israel, italy, japan, kuwait, latvia, lithuania, luxembourg, malaysia, malta, mexico, morocco, netherlands, new zealand, nigeria, norway, oman, pakistan, panama, peru, philippines, poland, portugal, qatar, romania, saudi arabia, singapore, slovakia, slovenia, south africa, south korea, spain, sweden, switzerland, taiwan, thailand, t√ºrkiye,turkey, ukraine, united arab emirates, uk,united kingdom, usa,us,united states, uruguay, venezuela, vietnam, usa/ca, worldwide. (Probablemente Error 429)\n",
      "Buscando: \"Machine Learning Engineer\" AND \"Future of Work\"\n",
      "‚úÖ LinkedIn encontr√≥ 26 trabajos.\n",
      "Buscando: \"Data Analyst\" AND \"Fintech\"\n",
      "‚úÖ LinkedIn encontr√≥ 50 trabajos.\n",
      "Buscando: \"Data Analyst\" AND \"EdTech\"\n",
      "‚úÖ LinkedIn encontr√≥ 20 trabajos.\n",
      "Buscando: \"Data Analyst\" AND \"Future of Work\"\n",
      "‚úÖ LinkedIn encontr√≥ 30 trabajos.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Scraper: LinkedIn (El \"Delicado\") ---\n",
    "# Alto riesgo de 429. Lo llamamos con cuidado.\n",
    "print(\"\\n--- Iniciando Scraper: LinkedIn ---\")\n",
    "for item in search_terms:\n",
    "    search_term = f'\"{item[0]}\" AND \"{item[1]}\"'\n",
    "    print(f\"Buscando: {search_term}\")\n",
    "    try:\n",
    "        linkedin_jobs = scrape_jobs(\n",
    "            site_name=[\"linkedin\"],\n",
    "            search_term=search_term,\n",
    "            location=location,\n",
    "            results_wanted=50, # MUY BAJO para evitar 429 sin proxies\n",
    "            hours_old=720,\n",
    "            linkedin_fetch_description=True # Clave para enriquecimiento\n",
    "        )\n",
    "        if linkedin_jobs is not None and not linkedin_jobs.empty:\n",
    "            print(f\"‚úÖ LinkedIn encontr√≥ {len(linkedin_jobs)} trabajos.\")\n",
    "            all_jobs_dfs.append(linkedin_jobs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en LinkedIn: {e}. (Probablemente Error 429)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fefd41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping secuencial completado ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Scraping secuencial completado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a9118f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ¬°√âxito! Se guardaron 327 trabajos √∫nicos en:\n",
      "../data/raw/jobs_20251115_171800/all_jobs_combined.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547176/2053293427.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_jobs = pd.concat(all_jobs_dfs, ignore_index=True).drop_duplicates(\n"
     ]
    }
   ],
   "source": [
    "if all_jobs_dfs:\n",
    "    combined_jobs = pd.concat(all_jobs_dfs, ignore_index=True).drop_duplicates(\n",
    "        subset=['job_url', 'title', 'company']\n",
    "    )\n",
    "    \n",
    "    # Esta l√≠nea ahora funciona, porque 'output_dir' es un objeto Path\n",
    "    combined_filename = f\"{output_dir }/all_jobs_combined.json\"\n",
    "    \n",
    "    combined_jobs.to_json(\n",
    "        combined_filename, \n",
    "        orient='records', \n",
    "        indent=4, \n",
    "        force_ascii=False\n",
    "    )\n",
    "    print(f\"\\nüéâ ¬°√âxito! Se guardaron {len(combined_jobs)} trabajos √∫nicos en:\")\n",
    "    print(combined_filename)\n",
    "else:\n",
    "    print(\"\\nNo se pudo descargar ning√∫n trabajo de ninguna fuente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market_scrapper_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
