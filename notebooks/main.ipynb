{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b23114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-jobspy in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (1.1.82)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2.3.3)\n",
      "Requirement already satisfied: NUMPY==1.26.3 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (1.26.3)\n",
      "Requirement already satisfied: tls-client<2.0.0,>=1.0.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (4.14.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.3.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2.12.4)\n",
      "Requirement already satisfied: markdownify<0.14.0,>=0.13.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (0.13.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2.32.5)\n",
      "Requirement already satisfied: regex<2025.0.0,>=2024.4.28 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from python-jobspy) (2024.11.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->python-jobspy) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->python-jobspy) (4.15.0)\n",
      "Requirement already satisfied: six<2,>=1.15 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from markdownify<0.14.0,>=0.13.1->python-jobspy) (1.17.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (2.41.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2025.11.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (4.67.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xlsxwriter in /home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/lib/python3.10/site-packages (3.2.9)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/wagner/.pyenv/versions/3.10.0/envs/market_scrapper_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install needed libraries\n",
    "%pip install -U python-jobspy\n",
    "%pip install tqdm\n",
    "%pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14676f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from jobspy import scrape_jobs\n",
    "from pathlib import Path \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "901c87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_directory(base_dir=\"../data/raw\"):\n",
    "    \"\"\"\n",
    "    Crea un directorio de salida con timestamp (usando pathlib) \n",
    "    y devuelve un objeto Path.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 2. Usar pathlib para crear la ruta\n",
    "    output_dir = Path(base_dir) / f\"jobs_{timestamp}\"\n",
    "    \n",
    "    # 3. Usar .mkdir() del objeto Path\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 4. Devolver el objeto Path (no un string)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4fbf1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de salida: ../data/raw/jobs_20251115_160526\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Definir Directorio de Salida ---\n",
    "output_dir = setup_output_directory()\n",
    "print(f\"Directorio de salida: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Definir Par√°metros de B√∫squeda Base ---\n",
    "roles_buscados = [\"Data Scientist\", \"Machine Learning Engineer\", \"Data Analyst\"]\n",
    "sectores_clave = [\"Fintech\", \"EdTech\", \"Future of Work\"]\n",
    "terminos_busqueda = list(product(roles_buscados, sectores_clave))\n",
    "search_term_base = f'({rol_buscado}) AND ({sectores_clave})'\n",
    "location = \"Remote\"\n",
    "country_code = \"USA\" # C√≥digo de pa√≠s para Indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c27f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par√°metros listos. Iniciaremos scrapers secuenciales y especializados.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Lista para guardar resultados ---\n",
    "# Guardaremos los DataFrames de cada sitio aqu√≠\n",
    "all_jobs_dfs = []\n",
    "\n",
    "print(\"Par√°metros listos. Iniciaremos scrapers secuenciales y especializados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef391493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: Indeed ---\n",
      "Buscando: (\"Data Scientist\" OR \"Machine Learning Engineer\" OR \"Data Analyst\") AND ((Fintech OR EdTech OR \"Future of Work\")) | Resultados: 200\n",
      "‚úÖ Indeed encontr√≥ 29 trabajos.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Scraper: Indeed (El \"Caballo de batalla\") ---\n",
    "# Es el m√°s estable y sin l√≠mites de solicitudes\n",
    "print(\"\\n--- Iniciando Scraper: Indeed ---\")\n",
    "print(f\"Buscando: {search_term_base} | Resultados: 200\")\n",
    "try:\n",
    "    indeed_jobs = scrape_jobs(\n",
    "        site_name=[\"indeed\"],\n",
    "        search_term=search_term_base + \" -marketing\", # Podemos afinar el t√©rmino\n",
    "        location=location,\n",
    "        country_indeed=country_code,\n",
    "        results_wanted=200, # Le pedimos m√°s porque es estable\n",
    "        hours_old=720\n",
    "    )\n",
    "    if indeed_jobs is not None and not indeed_jobs.empty:\n",
    "        print(f\"‚úÖ Indeed encontr√≥ {len(indeed_jobs)} trabajos.\")\n",
    "        all_jobs_dfs.append(indeed_jobs)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en Indeed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94b6d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: LinkedIn ---\n",
      "Buscando: (\"Data Scientist\" OR \"Machine Learning Engineer\" OR \"Data Analyst\") AND ((Fintech OR EdTech OR \"Future of Work\")) | Resultados: 50 (L√≠mite bajo sin proxy)\n",
      "‚úÖ LinkedIn encontr√≥ 50 trabajos.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Scraper: LinkedIn (El \"Delicado\") ---\n",
    "# Alto riesgo de 429. Lo llamamos con cuidado.\n",
    "print(\"\\n--- Iniciando Scraper: LinkedIn ---\")\n",
    "print(f\"Buscando: {search_term_base} | Resultados: 50 (L√≠mite bajo sin proxy)\")\n",
    "try:\n",
    "    linkedin_jobs = scrape_jobs(\n",
    "        site_name=[\"linkedin\"],\n",
    "        search_term=search_term_base,\n",
    "        location=location,\n",
    "        results_wanted=50, # MUY BAJO para evitar 429 sin proxies\n",
    "        hours_old=720,\n",
    "        linkedin_fetch_description=True # Clave para enriquecimiento\n",
    "    )\n",
    "    if linkedin_jobs is not None and not linkedin_jobs.empty:\n",
    "        print(f\"‚úÖ LinkedIn encontr√≥ {len(linkedin_jobs)} trabajos.\")\n",
    "        all_jobs_dfs.append(linkedin_jobs)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en LinkedIn: {e}. (Probablemente Error 429)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c5b5638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Scraper: ZipRecruiter ---\n",
      "Buscando: (\"Data Scientist\" OR \"Machine Learning Engineer\" OR \"Data Analyst\") AND ((Fintech OR EdTech OR \"Future of Work\")) | Resultados: 100\n",
      "\n",
      "--- Scraping secuencial completado ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Scraper: ZipRecruiter (El \"Est√°ndar\") ---\n",
    "print(\"\\n--- Iniciando Scraper: ZipRecruiter ---\")\n",
    "print(f\"Buscando: {search_term_base} | Resultados: 100\")\n",
    "try:\n",
    "    zip_jobs = scrape_jobs(\n",
    "        site_name=[\"zip_recruiter\"],\n",
    "        search_term=search_term_base,\n",
    "        location=location,\n",
    "        results_wanted=100, \n",
    "        hours_old=720\n",
    "    )\n",
    "    if zip_jobs is not None and not zip_jobs.empty:\n",
    "        print(f\"‚úÖ ZipRecruiter encontr√≥ {len(zip_jobs)} trabajos.\")\n",
    "        all_jobs_dfs.append(zip_jobs)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en ZipRecruiter: {e}\")\n",
    "\n",
    "print(\"\\n--- Scraping secuencial completado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a9118f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ¬°√âxito! Se guardaron 79 trabajos √∫nicos en:\n",
      "../data/raw/jobs_20251115_160526/all_jobs_combined.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_464921/2053293427.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_jobs = pd.concat(all_jobs_dfs, ignore_index=True).drop_duplicates(\n"
     ]
    }
   ],
   "source": [
    "if all_jobs_dfs:\n",
    "    combined_jobs = pd.concat(all_jobs_dfs, ignore_index=True).drop_duplicates(\n",
    "        subset=['job_url', 'title', 'company']\n",
    "    )\n",
    "    \n",
    "    # Esta l√≠nea ahora funciona, porque 'output_dir' es un objeto Path\n",
    "    combined_filename = f\"{output_dir }/all_jobs_combined.json\"\n",
    "    \n",
    "    combined_jobs.to_json(\n",
    "        combined_filename, \n",
    "        orient='records', \n",
    "        indent=4, \n",
    "        force_ascii=False\n",
    "    )\n",
    "    print(f\"\\nüéâ ¬°√âxito! Se guardaron {len(combined_jobs)} trabajos √∫nicos en:\")\n",
    "    print(combined_filename)\n",
    "else:\n",
    "    print(\"\\nNo se pudo descargar ning√∫n trabajo de ninguna fuente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market_scrapper_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
